{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80423ab-1845-47a7-9726-f236b3022893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, logging, pickle, joblib, sys, warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import ensemble, metrics, pipeline, preprocessing, impute, model_selection\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from shaphypetune import BoostRFE, BoostBoruta\n",
    "\n",
    "ML_RAW_PATH = \"/data2/zhoujb/project/cowpea_project/basedXPXLR/ML/rawData/\"\n",
    "TEST_RES_PATH = \"/data2/zhoujb/project/cowpea_project/basedXPXLR/ML/tesRes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92a0948-1cac-4f16-b415-cebf7a3fd3b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15598\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 512\n",
      "[LightGBM] [Info] Start training from score 12.519382\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 1.94141\tvalid_1's l2: 4.68651\n",
      "[20]\ttraining's l2: 1.21275\tvalid_1's l2: 4.17862\n",
      "[30]\ttraining's l2: 0.818753\tvalid_1's l2: 4.16434\n",
      "[40]\ttraining's l2: 0.55816\tvalid_1's l2: 4.00933\n",
      "[50]\ttraining's l2: 0.390461\tvalid_1's l2: 3.97357\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's l2: 0.450807\tvalid_1's l2: 3.95959\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15648\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 514\n",
      "[LightGBM] [Info] Start training from score 12.346826\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 1.81844\tvalid_1's l2: 4.92465\n",
      "[20]\ttraining's l2: 1.14778\tvalid_1's l2: 4.61556\n",
      "[30]\ttraining's l2: 0.766528\tvalid_1's l2: 4.43414\n",
      "[40]\ttraining's l2: 0.54012\tvalid_1's l2: 4.32238\n",
      "[50]\ttraining's l2: 0.385181\tvalid_1's l2: 4.3713\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l2: 0.472879\tvalid_1's l2: 4.29557\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15662\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 516\n",
      "[LightGBM] [Info] Start training from score 12.412190\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.08992\tvalid_1's l2: 4.76087\n",
      "[20]\ttraining's l2: 1.32179\tvalid_1's l2: 4.10277\n",
      "[30]\ttraining's l2: 0.903383\tvalid_1's l2: 4.1407\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttraining's l2: 1.17669\tvalid_1's l2: 4.04195\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003357 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15622\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 509\n",
      "[LightGBM] [Info] Start training from score 12.373107\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.15239\tvalid_1's l2: 4.25516\n",
      "[20]\ttraining's l2: 1.35788\tvalid_1's l2: 4.03068\n",
      "[30]\ttraining's l2: 0.915093\tvalid_1's l2: 4.09275\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttraining's l2: 1.35788\tvalid_1's l2: 4.03068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15619\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 509\n",
      "[LightGBM] [Info] Start training from score 12.307078\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.22961\tvalid_1's l2: 4.14078\n",
      "[20]\ttraining's l2: 1.35001\tvalid_1's l2: 3.68879\n",
      "[30]\ttraining's l2: 0.914794\tvalid_1's l2: 3.48928\n",
      "[40]\ttraining's l2: 0.642434\tvalid_1's l2: 3.49686\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's l2: 0.733696\tvalid_1's l2: 3.44063\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_table(os.path.join(ML_RAW_PATH, \"raw_data_PSugar.txt\"), sep=\"\\t\", index_col=0)\n",
    "\n",
    "target_col = ['GZ-PSugar']\n",
    "raw_data = raw_data.dropna(subset=target_col)\n",
    "\n",
    "feat_col = [x for x in raw_data.columns if x.startswith(\"fea_\")]\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True,  random_state=0)\n",
    "y_test_final, y_pred_final = [], []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(raw_data)):\n",
    "    data_train = raw_data.iloc[train_index].copy()\n",
    "    data_test = raw_data.iloc[test_index].copy()\n",
    "\n",
    "    scale_tool = preprocessing.StandardScaler()\n",
    "    scale_tool.fit(data_train.loc[:, feat_col])\n",
    "    data_train.loc[:, feat_col] = scale_tool.transform(data_train.loc[:, feat_col])\n",
    "    data_test.loc[:, feat_col] = scale_tool.transform(data_test.loc[:, feat_col])\n",
    "\n",
    "    train_sel = data_train.sample(frac=0.8, random_state=0)\n",
    "    val_sel = data_train.drop(train_sel.index).copy()\n",
    "\n",
    "    X_train = train_sel[feat_col].copy()\n",
    "    y_train = train_sel[target_col].values.ravel()\n",
    "\n",
    "    X_val = val_sel[feat_col].copy()\n",
    "    y_val = val_sel[target_col].values.ravel()\n",
    "\n",
    "    X_test = data_test[feat_col].copy()\n",
    "    y_test = data_test[target_col].values.ravel()\n",
    "\n",
    "    clf_model = lgb.LGBMRegressor(boosting_type=\"gbdt\", n_estimators=1000, random_state=0, n_jobs=4, num_leaves=5)\n",
    "\n",
    "    clf_model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                  callbacks=[lgb.log_evaluation(period=10), \n",
    "                             lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "    y_pred = clf_model.predict(X_test)\n",
    "\n",
    "    y_test_final.append(y_test)\n",
    "    y_pred_final.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe1a8bb-ecac-4dbe-8e13-471f15e570d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(columns=[\"Model\", \"Times\", \"Score\", \"Type\"])\n",
    "for i in range(len(y_test_final)):\n",
    "    ## Get score\n",
    "    #score_pear = pearsonr(y_test_final[i], y_pred_final[i])[0]\n",
    "    score_spear = spearmanr(y_test_final[i], y_pred_final[i])[0]\n",
    "    #score_rmse = metrics.mean_squared_error(y_test_final[i], y_pred_final[i], squared=False)\n",
    "    score_rmse = metrics.root_mean_squared_error(y_test_final[i], y_pred_final[i])\n",
    "    score_nrmse = score_rmse / np.std(y_test_final[i])\n",
    "\n",
    "    #res_df.loc[len(res_df)] = [\"LB_ALL\", i+1, score_pear, \"R\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_ALL\", i+1, score_spear, \"R\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_ALL\", i+1, score_rmse, \"RMSE\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_ALL\", i+1, score_nrmse, \"NRMSE\"]\n",
    "\n",
    "with open(os.path.join(TEST_RES_PATH, \"PSugar_LB_ALL.pickle\"), \"wb\") as out_f:\n",
    "    pickle.dump(res_df, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d55dff8-7cea-4577-abe4-71f48f112d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "NRMSE    0.796767\n",
       "R        0.625229\n",
       "RMSE     1.701180\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.groupby([\"Type\"])[\"Score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8675b882-f965-474b-963e-81fafe496014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
