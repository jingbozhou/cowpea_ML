{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b36878-e098-48aa-a729-26648f8ab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, logging, pickle, joblib, sys, warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import ensemble, metrics, pipeline, preprocessing, impute, model_selection\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "ML_RAW_PATH = \"/data2/zhoujb/project/cowpea_project/basedXPXLR/ML/rawData/\"\n",
    "FS_PATH = \"/data2/zhoujb/project/cowpea_project/basedXPXLR/ML/fs_PSugar/\"\n",
    "TEST_RES_PATH = \"/data2/zhoujb/project/cowpea_project/basedXPXLR/ML/tesRes/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d6ed0e-74de-4c4a-bbeb-99e02b52c84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003161 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13606\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 391\n",
      "[LightGBM] [Info] Start training from score 12.519382\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 1.94141\tvalid_1's l2: 4.68651\n",
      "[20]\ttraining's l2: 1.21275\tvalid_1's l2: 4.17862\n",
      "[30]\ttraining's l2: 0.818753\tvalid_1's l2: 4.16434\n",
      "[40]\ttraining's l2: 0.55816\tvalid_1's l2: 4.00933\n",
      "[50]\ttraining's l2: 0.392703\tvalid_1's l2: 3.96104\n",
      "Early stopping, best iteration is:\n",
      "[46]\ttraining's l2: 0.450807\tvalid_1's l2: 3.95959\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13652\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 394\n",
      "[LightGBM] [Info] Start training from score 12.346826\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 1.81844\tvalid_1's l2: 4.92465\n",
      "[20]\ttraining's l2: 1.14778\tvalid_1's l2: 4.61556\n",
      "[30]\ttraining's l2: 0.766528\tvalid_1's l2: 4.43414\n",
      "[40]\ttraining's l2: 0.54012\tvalid_1's l2: 4.32238\n",
      "[50]\ttraining's l2: 0.385551\tvalid_1's l2: 4.35776\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l2: 0.472879\tvalid_1's l2: 4.29557\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13671\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 396\n",
      "[LightGBM] [Info] Start training from score 12.412190\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.08992\tvalid_1's l2: 4.76087\n",
      "[20]\ttraining's l2: 1.32179\tvalid_1's l2: 4.10277\n",
      "[30]\ttraining's l2: 0.894402\tvalid_1's l2: 4.03934\n",
      "[40]\ttraining's l2: 0.618994\tvalid_1's l2: 4.04362\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttraining's l2: 0.687757\tvalid_1's l2: 3.99393\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13642\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 391\n",
      "[LightGBM] [Info] Start training from score 12.373107\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.15239\tvalid_1's l2: 4.25516\n",
      "[20]\ttraining's l2: 1.35788\tvalid_1's l2: 4.03068\n",
      "[30]\ttraining's l2: 0.908147\tvalid_1's l2: 4.02457\n",
      "[40]\ttraining's l2: 0.634101\tvalid_1's l2: 3.95367\n",
      "[50]\ttraining's l2: 0.44794\tvalid_1's l2: 3.93867\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's l2: 0.548863\tvalid_1's l2: 3.91713\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002328 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13623\n",
      "[LightGBM] [Info] Number of data points in the train set: 192, number of used features: 389\n",
      "[LightGBM] [Info] Start training from score 12.307078\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\ttraining's l2: 2.22961\tvalid_1's l2: 4.14078\n",
      "[20]\ttraining's l2: 1.35001\tvalid_1's l2: 3.68879\n",
      "[30]\ttraining's l2: 0.914794\tvalid_1's l2: 3.48928\n",
      "[40]\ttraining's l2: 0.642434\tvalid_1's l2: 3.49686\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's l2: 0.733696\tvalid_1's l2: 3.44063\n"
     ]
    }
   ],
   "source": [
    "feat_col = []\n",
    "with open(os.path.join(FS_PATH, \"lgb_rfe_cv_psugar_rmse\")) as f:\n",
    "    for line in f:\n",
    "        feat_col.append(line.strip())\n",
    "\n",
    "raw_data = pd.read_table(os.path.join(ML_RAW_PATH, \"raw_data_PSugar.txt\"), sep=\"\\t\", index_col=0)\n",
    "\n",
    "target_col = ['GZ-PSugar']\n",
    "raw_data = raw_data.dropna(subset=target_col)\n",
    "\n",
    "#cat_col_names_raw = [\"heading_stage_d\", \"leaf_blast_rep1\", \"leaf_blast_rep2\", \"leaf_blast_average\", \"GR_D3\", \"GR_D2\", \"GR_D1\"]\n",
    "#cat_col_names_raw = [\"feat_{}\".format(x) for x in cat_col_names_raw]\n",
    "#cat_col_names = [i for i in feat_col if i in cat_col_names_raw]\n",
    "#num_col_names = [i for i in feat_col if (i.startswith(\"feat\")) and (i not in cat_col_names)]\n",
    "#cat_col_num = []\n",
    "#for item in cat_col_names:\n",
    "    #cat_col_num.append(feat_col.index(item))\n",
    "\n",
    "#for col in cat_col_names:\n",
    "    #raw_data[col] = raw_data[col].astype(\"str\")\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True,  random_state=0)\n",
    "y_test_final, y_pred_final = [], []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(raw_data)):\n",
    "    data_train = raw_data.iloc[train_index].copy()\n",
    "    data_test = raw_data.iloc[test_index].copy()\n",
    "    \n",
    "    scale_tool = preprocessing.StandardScaler()\n",
    "    scale_tool.fit(data_train.loc[:, feat_col])\n",
    "    data_train.loc[:, feat_col] = scale_tool.transform(data_train.loc[:, feat_col])\n",
    "    data_test.loc[:, feat_col] = scale_tool.transform(data_test.loc[:, feat_col])\n",
    "\n",
    "    train_sel = data_train.sample(frac=0.8, random_state=0)\n",
    "    val_sel = data_train.drop(train_sel.index).copy()\n",
    "\n",
    "    X_train = train_sel[feat_col].copy()\n",
    "    y_train = train_sel[target_col].values.ravel()\n",
    "\n",
    "    X_val = val_sel[feat_col].copy()\n",
    "    y_val = val_sel[target_col].values.ravel()\n",
    "\n",
    "    X_test = data_test[feat_col].copy()\n",
    "    y_test = data_test[target_col].values.ravel()\n",
    "\n",
    "    # Initialize CatBoostClassifier\n",
    "    clf_model = lgb.LGBMRegressor(boosting_type=\"gbdt\", n_estimators=1000, random_state=0, n_jobs=4, num_leaves=5)\n",
    "\n",
    "    clf_model.fit(X_train, y_train, \n",
    "                  eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                  callbacks=[lgb.log_evaluation(period=10), \n",
    "                             lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "    y_pred = clf_model.predict(X_test)\n",
    "\n",
    "    y_test_final.append(y_test)\n",
    "    y_pred_final.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4670362e-f754-4092-bed1-99708d949308",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(columns=[\"Model\", \"Times\", \"Score\", \"Type\"])\n",
    "for i in range(len(y_test_final)):\n",
    "    ## Get score\n",
    "    #score_pear = pearsonr(y_test_final[i], y_pred_final[i])[0]\n",
    "    score_spear = spearmanr(y_test_final[i], y_pred_final[i])[0]\n",
    "    #score_rmse = metrics.mean_squared_error(y_test_final[i], y_pred_final[i], squared=False)\n",
    "    score_rmse = metrics.root_mean_squared_error(y_test_final[i], y_pred_final[i])\n",
    "    score_nrmse = score_rmse / np.std(y_test_final[i])\n",
    "\n",
    "    #res_df.loc[len(res_df)] = [\"LB_RFE\", i+1, score_pear, \"R\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_RFE\", i+1, score_spear, \"R\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_RFE\", i+1, score_rmse, \"RMSE\"]\n",
    "    res_df.loc[len(res_df)] = [\"LB_RFE\", i+1, score_nrmse, \"NRMSE\"]\n",
    "\n",
    "with open(os.path.join(TEST_RES_PATH, \"PSugar_LB_RFE.pickle\"), \"wb\") as out_f:\n",
    "    pickle.dump(res_df, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4b8054-d956-4088-bba4-bb720d2f50fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "NRMSE    0.792557\n",
       "R        0.622228\n",
       "RMSE     1.691994\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.groupby([\"Type\"])[\"Score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d682766-2c2f-4c09-8cea-6686fee88323",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
